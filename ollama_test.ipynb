{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMcRqU+vVB3Xo3OeCBcJmsI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aurotripathy/RL-Experiments/blob/main/ollama_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "to run ollama, you need to know what port its running on\n",
        " 'model_server': 'http://localhost:11434/v1',  # Ollama"
      ],
      "metadata": {
        "id": "P1W7vntbE0rF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-xterm\n",
        "%load_ext colabxterm"
      ],
      "metadata": {
        "id": "cKrw0qcc8ShC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%xterm"
      ],
      "metadata": {
        "id": "YPQ-2BLv8dxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "curl https://ollama.ai/install.sh | sh\n",
        "ollama serve &"
      ],
      "metadata": {
        "id": "amNfkVgLEgk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qwen-agent[gui,rag,code_interpreter,mcp]\n",
        "!pip install qwen-agent"
      ],
      "metadata": {
        "id": "BGENYUM56MZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from https://github.com/QwenLM/Qwen-Agent\n",
        "\n",
        "import pprint\n",
        "import urllib.parse\n",
        "import json5\n",
        "from qwen_agent.agents import Assistant\n",
        "from qwen_agent.tools.base import BaseTool, register_tool\n",
        "from qwen_agent.utils.output_beautify import typewriter_print"
      ],
      "metadata": {
        "id": "xOOpjPoW-G1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 (Optional): Add a custom tool named `my_image_gen`.\n",
        "@register_tool('my_image_gen')\n",
        "class MyImageGen(BaseTool):\n",
        "    # The `description` tells the agent the functionality of this tool.\n",
        "    description = 'AI painting (image generation) service, input text description, and return the image URL drawn based on text information.'\n",
        "    # The `parameters` tell the agent what input parameters the tool has.\n",
        "    parameters = [{\n",
        "        'name': 'prompt',\n",
        "        'type': 'string',\n",
        "        'description': 'Detailed description of the desired image content, in English',\n",
        "        'required': True\n",
        "    }]\n",
        "\n",
        "    def call(self, params: str, **kwargs) -> str:\n",
        "        # `params` are the arguments generated by the LLM agent.\n",
        "        prompt = json5.loads(params)['prompt']\n",
        "        prompt = urllib.parse.quote(prompt)\n",
        "        return json5.dumps(\n",
        "            {'image_url': f'https://image.pollinations.ai/prompt/{prompt}'},\n",
        "            ensure_ascii=False)"
      ],
      "metadata": {
        "id": "aaVHD5tc-XfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Configure the LLM you are using.\n",
        "llm_cfg = {\n",
        "    # Use the model service provided by DashScope:\n",
        "    # 'model': 'qwen-max-latest',\n",
        "    # 'model_type': 'qwen_dashscope',\n",
        "    # 'api_key': 'YOUR_DASHSCOPE_API_KEY',\n",
        "    # It will use the `DASHSCOPE_API_KEY' environment variable if 'api_key' is not set here.\n",
        "\n",
        "    # Use a model service compatible with the OpenAI API, such as vLLM or Ollama:\n",
        "    'model': 'qwen3:8b',\n",
        "    # 'model_server': 'http://localhost:8000/v1',  # base_url, also known as api_base\n",
        "    'model_server': 'http://localhost:11434/v1',  # Ollama\n",
        "    'api_key': 'EMPTY',\n",
        "\n",
        "    # (Optional) LLM hyperparameters for generation:\n",
        "    'generate_cfg': {\n",
        "        'top_p': 0.8\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "Nm7AP1h3-dGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Create an agent. Here we use the `Assistant` agent as an example, which is capable of using tools and reading files.\n",
        "system_instruction = '''After receiving the user's request, you should:\n",
        "- first draw an image and obtain the image url,\n",
        "- then run code `request.get(image_url)` to download the image,\n",
        "- and finally select an image operation from the given document to process the image.\n",
        "Please show the image using `plt.show()`.'''\n",
        "tools = ['my_image_gen', 'code_interpreter']  # `code_interpreter` is a built-in tool for executing code.\n",
        "files = []  # Removed 'Zheng2024_LargeLanguageModelsinDrugDiscovery.pdf' as it is not needed for this task.\n",
        "bot = Assistant(llm=llm_cfg,\n",
        "                system_message=system_instruction,\n",
        "                function_list=tools,)"
      ],
      "metadata": {
        "id": "h-R5uHp5-jRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Run the agent as a chatbot.\n",
        "messages = []  # This stores the chat history.\n",
        "while True:\n",
        "    # For example, enter the query \"draw a dog and rotate it 90 degrees\".\n",
        "    query = input('\\nuser query: ')\n",
        "    # Append the user query to the chat history.\n",
        "    messages.append({'role': 'user', 'content': query})\n",
        "    response = []\n",
        "    response_plain_text = ''\n",
        "    print('bot response:')\n",
        "    for response in bot.run(messages=messages):\n",
        "        # Streaming output.\n",
        "        response_plain_text = typewriter_print(response, response_plain_text)\n",
        "    # Append the bot responses to the chat history.\n",
        "    messages.extend(response)\n"
      ],
      "metadata": {
        "id": "LagExFuT-ndv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-jFauRcJ-zhv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}