{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMZsom0DXNBDf+OStWGuRNI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aurotripathy/RL-Experiments/blob/main/qwen3_assistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If this is Colab, ensure you are using a GPU\n",
        "\n",
        "https://github.com/QwenLM/Qwen-Agent/blob/main/examples/assistant_qwen3.py"
      ],
      "metadata": {
        "id": "VZcv16u3FPub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"qwen-agent[gui,rag,code_interpreter,mcp]\"\n",
        "# Or use `pip install -U qwen-agent` for the minimal requirements.\n",
        "# The optional requirements, specified in double brackets, are:\n",
        "#   [gui] for Gradio-based GUI support;\n",
        "#   [rag] for RAG support;\n",
        "#   [code_interpreter] for Code Interpreter support;\n",
        "#   [mcp] for MCP support."
      ],
      "metadata": {
        "id": "PM4iqkZ-GBk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://medium.com/@abonia/running-ollama-in-google-colab-free-tier-545609258453\n",
        "!pip install colab-xterm\n",
        "%load_ext colabxterm"
      ],
      "metadata": {
        "id": "JHE4Qs1WG5bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%xterm"
      ],
      "metadata": {
        "id": "b8ryf3kFHGP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7moegd23FKuB"
      },
      "outputs": [],
      "source": [
        "\"\"\"An agent implemented by assistant with qwen3\"\"\"\n",
        "import os  # noqa\n",
        "\n",
        "from qwen_agent.agents import Assistant\n",
        "from qwen_agent.gui import WebUI\n",
        "from qwen_agent.utils.output_beautify import typewriter_print"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def init_agent_service():\n",
        "\n",
        "    llm_cfg = {\n",
        "        # Use your own model service compatible with OpenAI API by vLLM/SGLang:\n",
        "        'model': 'Qwen/Qwen3-8B',\n",
        "        'model_server': 'http://localhost:11434',  # api_base\n",
        "        'api_key': 'EMPTY',\n",
        "\n",
        "        'generate_cfg': {\n",
        "            # When using vLLM/SGLang OAI API, pass the parameter of whether to enable thinking mode in this way\n",
        "            'extra_body': {\n",
        "                'chat_template_kwargs': {'enable_thinking': False}\n",
        "            },\n",
        "\n",
        "            # Add: When the content is `<think>this is the thought</think>this is the answer`\n",
        "            # Do not add: When the response has been separated by reasoning_content and content\n",
        "            # This parameter will affect the parsing strategy of tool call\n",
        "            # 'thought_in_content': True,\n",
        "        },\n",
        "    }\n",
        "    tools = [\n",
        "        # {\n",
        "        #     'mcpServers': {  # You can specify the MCP configuration file\n",
        "        #         'time': {\n",
        "        #             'command': 'uvx',\n",
        "        #             'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n",
        "        #         },\n",
        "        #         'fetch': {\n",
        "        #             'command': 'uvx',\n",
        "        #             'args': ['mcp-server-fetch']\n",
        "        #         }\n",
        "        #     }\n",
        "        # },\n",
        "        'code_interpreter',  # Built-in tools\n",
        "    ]\n",
        "    bot = Assistant(llm=llm_cfg,\n",
        "                    function_list=tools,\n",
        "                    name='Qwen3 Tool-calling Demo',\n",
        "                    description=\"I'm a demo using the Qwen3 tool calling. Welcome to add and play with your own tools!\")\n",
        "\n",
        "    return bot"
      ],
      "metadata": {
        "id": "8D4KkVoSFcfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(query: str = 'What time is it?'):\n",
        "    # Define the agent\n",
        "    bot = init_agent_service()\n",
        "\n",
        "    # Chat\n",
        "    messages = [{'role': 'user', 'content': query}]\n",
        "    response_plain_text = ''\n",
        "    for response in bot.run(messages=messages):\n",
        "        response_plain_text = typewriter_print(response, response_plain_text)\n",
        "\n",
        "\n",
        "def app_tui():\n",
        "    # Define the agent\n",
        "    bot = init_agent_service()\n",
        "\n",
        "    # Chat\n",
        "    messages = []\n",
        "    while True:\n",
        "        query = input('user question: ')\n",
        "        messages.append({'role': 'user', 'content': query})\n",
        "        response = []\n",
        "        response_plain_text = ''\n",
        "        for response in bot.run(messages=messages):\n",
        "            response_plain_text = typewriter_print(response, response_plain_text)\n",
        "        messages.extend(response)\n",
        "\n",
        "\n",
        "def app_gui():\n",
        "    # Define the agent\n",
        "    bot = init_agent_service()\n",
        "    chatbot_config = {\n",
        "        'prompt.suggestions': [\n",
        "            'What time is it?',\n",
        "            'https://github.com/orgs/QwenLM/repositories Extract markdown content of this page, then draw a bar chart to display the number of stars.'\n",
        "        ]\n",
        "    }\n",
        "    WebUI(\n",
        "        bot,\n",
        "        chatbot_config=chatbot_config,\n",
        "    ).run()\n",
        "\n"
      ],
      "metadata": {
        "id": "np18GlfAGW8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # test()\n",
        "    # app_tui()\n",
        "    app_gui()"
      ],
      "metadata": {
        "id": "6rnaIioiGhlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n"
      ],
      "metadata": {
        "id": "NURb7g6tGj16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nvD5Vi3JM5MX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}