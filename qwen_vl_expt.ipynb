{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOsRNfOYksEfcSZEgs601GO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aurotripathy/RL-Experiments/blob/main/qwen_vl_expt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Ensure you change the runtime to A100 - 80GB"
      ],
      "metadata": {
        "id": "fnYzToaYe67I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers"
      ],
      "metadata": {
        "id": "I8RHPEf3fK7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "K6M3MTjPfgzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import Qwen3VLMoeForConditionalGeneration, AutoProcessor\n",
        "import torch\n",
        "\n",
        "# default: Load the model on the available device(s)\n",
        "model = Qwen3VLMoeForConditionalGeneration.from_pretrained(\n",
        "    \"Qwen/Qwen3-VL-30B-A3B-Instruct\", dtype=\"auto\", device_map=\"cuda\"\n",
        ")\n",
        "\n",
        "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
        "# model = Qwen3VLMoeForConditionalGeneration.from_pretrained(\n",
        "#     \"Qwen/Qwen3-VL-30B-A3B-Instruct\",\n",
        "#     dtype=torch.bfloat16,\n",
        "#     attn_implementation=\"flash_attention_2\",\n",
        "#     device_map=\"auto\",\n",
        "# )\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-30B-A3B-Instruct\")\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\n",
        "                \"type\": \"image\",\n",
        "                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
        "            },\n",
        "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
        "        ],\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "# Preparation for inference\n",
        "inputs = processor.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,\n",
        "    return_dict=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(\"cuda\")"
      ],
      "metadata": {
        "id": "fVSN5Iyidf5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1089e14"
      },
      "source": [
        "from IPython.display import Image, display\n",
        "import requests\n",
        "\n",
        "# Assuming the image is the first item in the first message's content\n",
        "image_url = messages[0]['content'][0]['image']\n",
        "\n",
        "# Download the image from the URL\n",
        "response = requests.get(image_url)\n",
        "with open(\"temp_image.jpeg\", \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "# Display the image\n",
        "display(Image(filename=\"temp_image.jpeg\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference: Generation of the output\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
        "generated_ids_trimmed = [\n",
        "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "]\n",
        "output_text = processor.batch_decode(\n",
        "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        ")\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "Tj80N3YrhXiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J147j5DCd8Ja"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}